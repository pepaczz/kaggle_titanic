{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import IPython\n",
    "from IPython import display\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "    \n",
    "class EmbarkedImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        return None\n",
    "    def fit(self, X):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        # deep copy the df\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Clean up fares.\n",
    "        value_to_input = df.loc[(df['Fare'] < 85) & (df['Fare'] > 75)  & (df['Pclass'] == 1)]['Embarked'].mode()\n",
    "\n",
    "        value_to_input = value_to_input[0]\n",
    "\n",
    "        df.loc[(df['Embarked'].isnull()),['Embarked']] = value_to_input\n",
    "\n",
    "        return(df)\n",
    "    \n",
    "    \n",
    "class GeneralImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col_impute, col_group, impute_method = 'median'): # no *args or **kargs\n",
    "        self.col_impute = col_impute\n",
    "        self.col_group = col_group\n",
    "        self.impute_method = impute_method\n",
    "        return None\n",
    "    def fit(self, X):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        # deep copy the df because of transform\n",
    "        df = X.copy()\n",
    "\n",
    "        # Create a groupby object: by_sex_class\n",
    "        grouped = df.groupby(self.col_group)\n",
    "\n",
    "        # function to impute median\n",
    "        def imputer_median(series):\n",
    "            return series.fillna(series.median())\n",
    "        # function to impute average\n",
    "        def imputer_average(series):\n",
    "            return series.fillna(series.mean())\n",
    "\n",
    "        if self.impute_method == 'median':\n",
    "            # impute median\n",
    "            df[self.col_impute] = grouped[self.col_impute].transform(imputer_median)\n",
    "            return(df)\n",
    "        elif self.impute_method == 'average':\n",
    "            # impute average\n",
    "            df[self.col_impute] = grouped[self.col_impute].transform(imputer_average)\n",
    "            return(df)\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "        \n",
    "class TitleCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        return None\n",
    "    def fit(self, X):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        # deep copy the df because of transform\n",
    "        df = X.copy()\n",
    "\n",
    "        df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "        df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "        df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "        df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "        df['Title'] = df['Title'].fillna(np.nan) \n",
    "\n",
    "        return(df)\n",
    "    \n",
    "\n",
    "CAT_ATTRIBS = ['Sex','Embarked','Title']\n",
    "NUMERICS_ATTRIBS = ['Pclass','Age','SibSp','Parch','Fare']\n",
    "\n",
    "#Read train and test data\n",
    "train = pd.read_csv(\"../input/train.csv\", dtype={\"Age\": np.float64}, )\n",
    "test = pd.read_csv(\"../input/test.csv\", dtype={\"Age\": np.float64}, )\n",
    "\n",
    "\n",
    "my_mapper = DataFrameMapper([\n",
    "    ('Sex', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('Embarked', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('Title', sklearn.preprocessing.LabelBinarizer())\n",
    "    ], input_df=True)\n",
    "\n",
    "\n",
    "categorical_data_pipeline = Pipeline([\n",
    "    ('ebarked_imputer', EmbarkedImputer()),\n",
    "    ('title_creator', TitleCreator()),\n",
    "    ('label_binarizer_df', my_mapper),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "numerical_data_pipeline = Pipeline([\n",
    "    ('fare_imputer', GeneralImputer(col_impute=['Fare'], \n",
    "                                                        col_group=['Sex', 'Pclass'], \n",
    "                                                        impute_method='median')),\n",
    "    ('age_imputer', GeneralImputer(col_impute=['Age'], \n",
    "                                                       col_group=['Sex', 'Pclass'], \n",
    "                                                       impute_method='average')), # median perhaps?\n",
    "    ('selector', DataFrameSelector(NUMERICS_ATTRIBS)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", numerical_data_pipeline),\n",
    "    (\"cat_pipeline\", categorical_data_pipeline),\n",
    "])    \n",
    "\n",
    "    \n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(train)\n",
    "test_prepared = full_pipeline.fit_transform(test)\n",
    "\n",
    "# ##########################################\n",
    "# # USE TPOT TO FIND A CLASSIFIER\n",
    "# from tpot import TPOTClassifier\n",
    "# tpot = TPOTClassifier(verbosity=2, max_time_mins=10)\n",
    "# tpot.fit(train_prepared, train['Survived'])\n",
    "# ##########################################\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [\n",
    "    {'bootstrap': [False, True], \n",
    "     'n_estimators': [80, 100, 130], \n",
    "     'max_features': [0.65, 0.7500000000000001],\n",
    "     'min_samples_leaf': [10,12], \n",
    "     'min_samples_split': [3,5,7] \n",
    "    },\n",
    "]\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_classifier, param_grid, cv=5,scoring='neg_mean_squared_error', refit=True)\n",
    "grid_search.fit(train_prepared, train['Survived'])\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "final_predictions = final_model.predict(test_prepared)\n",
    "# final_predictions\n",
    "\n",
    "my_submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': final_predictions})\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
