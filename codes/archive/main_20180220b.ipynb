{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#load packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import IPython\n",
    "from IPython import display\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "\n",
    "import titanic_functions as titfun\n",
    "import transformer_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_ATTRIBS = ['Sex','Embarked','Title']\n",
    "NUMERICS_ATTRIBS = ['Pclass','Age','SibSp','Parch','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train and test data\n",
    "train = pd.read_csv(\"../input/train.csv\", dtype={\"Age\": np.float64}, )\n",
    "test = pd.read_csv(\"../input/test.csv\", dtype={\"Age\": np.float64}, )\n",
    "# combine into two\n",
    "combine = [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mapper = DataFrameMapper([\n",
    "    ('Sex', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('Embarked', sklearn.preprocessing.LabelBinarizer()),\n",
    "    ('Title', sklearn.preprocessing.LabelBinarizer())\n",
    "    ], input_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(891, 14)\n",
      "[[ 0.82737724 -0.55136635  0.43279337 ...,  1.          0.          0.        ]\n",
      " [-1.56610693  0.65402951  0.43279337 ...,  0.          1.          0.        ]\n",
      " [ 0.82737724 -0.25001739 -0.4745452  ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.82737724 -0.57020066  0.43279337 ...,  0.          0.          0.        ]\n",
      " [-1.56610693 -0.25001739 -0.4745452  ...,  1.          0.          0.        ]\n",
      " [ 0.82737724  0.20200606 -0.4745452  ...,  1.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "categorical_data_pipeline = Pipeline([\n",
    "    ('ebarked_imputer', transformer_classes.EmbarkedImputer()),\n",
    "    ('title_creator', transformer_classes.TitleCreator()),\n",
    "    ('label_binarizer_df', my_mapper),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "numerical_data_pipeline = Pipeline([\n",
    "    ('fare_imputer', transformer_classes.GeneralImputer(col_impute=['Fare'], \n",
    "                                                        col_group=['Sex', 'Pclass'], \n",
    "                                                        impute_method='median')),\n",
    "    ('age_imputer', transformer_classes.GeneralImputer(col_impute=['Age'], \n",
    "                                                       col_group=['Sex', 'Pclass'], \n",
    "                                                       impute_method='average')), # median perhaps?\n",
    "    ('selector', transformer_classes.DataFrameSelector(NUMERICS_ATTRIBS)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", numerical_data_pipeline),\n",
    "    (\"cat_pipeline\", categorical_data_pipeline),\n",
    "])    \n",
    "\n",
    "    \n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(train)\n",
    "test_prepared = full_pipeline.transform(test)\n",
    "\n",
    "print(type(train_prepared))\n",
    "print(train_prepared.shape)\n",
    "print(train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8417719849052429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.8417719849052429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.8417719849052429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.8429208305450022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.8429208305450022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.8440317300562065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.8440317300562065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.848570335277097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.00323805 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LinearSVC(RandomForestClassifier(RobustScaler(input_matrix), bootstrap=False, criterion=entropy, max_features=0.8, min_samples_leaf=12, min_samples_split=16, n_estimators=100), C=15.0, dual=True, loss=squared_hinge, penalty=l2, tol=0.0001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.preprocessing.Normalizer': {'norm': ['l1', 'l2', 'max']}, 'xgboost.XGBClassifier': {'n_estimators': [100], 'min_child_weight': range(1, 21), 'max_depth': range(1, 11), 'nthread': [1], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'subsample': array([ 0.05,  0.1 ,  0.15,  0.2 ,...l': [1e-05, 0.0001, 0.001, 0.01, 0.1], 'penalty': ['l1', 'l2'], 'loss': ['hinge', 'squared_hinge']}},\n",
       "        crossover_rate=0.1, cv=5, disable_update_check=False,\n",
       "        early_stop=None, generations=1000000, max_eval_time_mins=5,\n",
       "        max_time_mins=20, memory=None, mutation_rate=0.9, n_jobs=1,\n",
       "        offspring_size=100, periodic_checkpoint_folder=None,\n",
       "        population_size=100, random_state=None, scoring=None,\n",
       "        subsample=1.0, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(verbosity=2, max_time_mins=20)\n",
    "tpot.fit(train_prepared, train['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.export('tpot_titanic_pipeline_4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# param_grid = [\n",
    "#     {'bootstrap': [False, True], \n",
    "#      'n_estimators': [80, 100, 130], \n",
    "#      'max_features': [0.65, 0.7500000000000001],\n",
    "#      'min_samples_leaf': [10,12], \n",
    "#      'min_samples_split': [5] \n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "# grid_search = GridSearchCV(random_forest_classifier, param_grid, cv=5,scoring='neg_mean_squared_error', refit=True)\n",
    "# grid_search.fit(train_prepared, train['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    RobustScaler(),\n",
    "    StackingEstimator(estimator=RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.8, min_samples_leaf=12, min_samples_split=16, n_estimators=100)),\n",
    "    LinearSVC(C=15.0, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=0.0001)\n",
    ")\n",
    "# RobustScaler / StackingEstimator / LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(train_prepared, train['Survived'])\n",
    "final_predictions = exported_pipeline.predict(test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_features': 0.7500000000000001,\n",
       " 'min_samples_leaf': 12,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_search.best_params_\n",
    "# final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_predictions = final_model.predict(test_prepared)\n",
    "# final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': final_predictions})\n",
    "my_submission.to_csv('20180220b_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
